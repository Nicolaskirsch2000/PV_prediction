{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8435551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6769f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with (open(\"saved_results/clients_data\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            data.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90360467",
   "metadata": {},
   "source": [
    "data[customer][train_test][feature]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7957a93",
   "metadata": {},
   "source": [
    "## Dataframe creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7541fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One dataframe per household\n",
    "dfs_train = [None]*25\n",
    "dfs_test = [None]*25\n",
    "\n",
    "for i in range(len(data[0])):\n",
    "    #Create train dfs\n",
    "    mat_train = np.matrix(data[0][i][0])\n",
    "    dfs_train[i] = pd.DataFrame(mat_train)\n",
    "    dfs_train[i]['pred'] = data[0][i][1].tolist()\n",
    "    \n",
    "    #Create test dfs\n",
    "    mat = np.matrix(data[0][i][2])\n",
    "    dfs_test[i] = pd.DataFrame(mat)\n",
    "    dfs_test[i]['pred'] = data[0][i][3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e0b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One complete df \n",
    "tot_train = dfs_train[0]\n",
    "tot_test = dfs_test[0]\n",
    "\n",
    "for i in range(len(dfs_train)-1):\n",
    "    tot_train = tot_train.append(dfs_train[i+1])\n",
    "    tot_test = tot_test.append(dfs_train[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaeda20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.504165</td>\n",
       "      <td>-2.087780</td>\n",
       "      <td>-0.105168</td>\n",
       "      <td>1.668497</td>\n",
       "      <td>-1.432883</td>\n",
       "      <td>-1.387153</td>\n",
       "      <td>-0.639071</td>\n",
       "      <td>-0.126823</td>\n",
       "      <td>-0.367084</td>\n",
       "      <td>0.168863</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.541251</td>\n",
       "      <td>0.343220</td>\n",
       "      <td>0.061374</td>\n",
       "      <td>-1.206293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.470133</td>\n",
       "      <td>-1.814397</td>\n",
       "      <td>-0.105168</td>\n",
       "      <td>1.668497</td>\n",
       "      <td>-1.345920</td>\n",
       "      <td>-1.445318</td>\n",
       "      <td>-0.639071</td>\n",
       "      <td>-0.126823</td>\n",
       "      <td>-0.367084</td>\n",
       "      <td>-0.548628</td>\n",
       "      <td>-0.322581</td>\n",
       "      <td>-0.383017</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>-0.055306</td>\n",
       "      <td>-1.259082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.777487</td>\n",
       "      <td>-1.625675</td>\n",
       "      <td>-0.163278</td>\n",
       "      <td>1.668497</td>\n",
       "      <td>-1.399496</td>\n",
       "      <td>-1.355088</td>\n",
       "      <td>-0.639071</td>\n",
       "      <td>-0.126823</td>\n",
       "      <td>-0.367084</td>\n",
       "      <td>-0.548628</td>\n",
       "      <td>-0.701679</td>\n",
       "      <td>-0.624644</td>\n",
       "      <td>-0.716369</td>\n",
       "      <td>-0.330465</td>\n",
       "      <td>-1.344445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.376241</td>\n",
       "      <td>-1.484575</td>\n",
       "      <td>-0.163278</td>\n",
       "      <td>1.668497</td>\n",
       "      <td>-1.486133</td>\n",
       "      <td>-1.410676</td>\n",
       "      <td>-0.639071</td>\n",
       "      <td>-0.126823</td>\n",
       "      <td>-0.367084</td>\n",
       "      <td>-0.548628</td>\n",
       "      <td>-0.701679</td>\n",
       "      <td>-0.901689</td>\n",
       "      <td>-0.915552</td>\n",
       "      <td>-1.034742</td>\n",
       "      <td>-1.383241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.190888</td>\n",
       "      <td>-1.431662</td>\n",
       "      <td>-0.434455</td>\n",
       "      <td>1.668497</td>\n",
       "      <td>-1.525507</td>\n",
       "      <td>-1.500567</td>\n",
       "      <td>-0.196552</td>\n",
       "      <td>-0.126823</td>\n",
       "      <td>-0.367084</td>\n",
       "      <td>-0.548628</td>\n",
       "      <td>-0.701679</td>\n",
       "      <td>-0.901689</td>\n",
       "      <td>-1.143931</td>\n",
       "      <td>-1.218857</td>\n",
       "      <td>-1.191089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.504165 -2.087780 -0.105168  1.668497 -1.432883 -1.387153 -0.639071   \n",
       "1 -0.470133 -1.814397 -0.105168  1.668497 -1.345920 -1.445318 -0.639071   \n",
       "2 -0.777487 -1.625675 -0.163278  1.668497 -1.399496 -1.355088 -0.639071   \n",
       "3 -1.376241 -1.484575 -0.163278  1.668497 -1.486133 -1.410676 -0.639071   \n",
       "4 -2.190888 -1.431662 -0.434455  1.668497 -1.525507 -1.500567 -0.196552   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.126823 -0.367084  0.168863  0.008052  0.541251  0.343220  0.061374   \n",
       "1 -0.126823 -0.367084 -0.548628 -0.322581 -0.383017  0.045543 -0.055306   \n",
       "2 -0.126823 -0.367084 -0.548628 -0.701679 -0.624644 -0.716369 -0.330465   \n",
       "3 -0.126823 -0.367084 -0.548628 -0.701679 -0.901689 -0.915552 -1.034742   \n",
       "4 -0.126823 -0.367084 -0.548628 -0.701679 -0.901689 -1.143931 -1.218857   \n",
       "\n",
       "       pred  \n",
       "0 -1.206293  \n",
       "1 -1.259082  \n",
       "2 -1.344445  \n",
       "3 -1.383241  \n",
       "4 -1.191089  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea30e8",
   "metadata": {},
   "source": [
    "## Linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce1524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2 : 0.7495491609617009\n",
      "MSE : 0.28565978932868724\n"
     ]
    }
   ],
   "source": [
    "#One household\n",
    "X_train = dfs_train[0].loc[:, dfs_train[0].columns != 'pred']\n",
    "X_test = dfs_test[0].loc[:, dfs_test[0].columns != 'pred']\n",
    "\n",
    "y_train = dfs_train[0].loc[:, dfs_train[0].columns == 'pred']\n",
    "y_test = dfs_test[0].loc[:, dfs_test[0].columns == 'pred']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cde7745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2 : 0.68700508300428\n",
      "MSE : 0.31843413883381044+- 12.618981223380832%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4be95dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2 : 0.7878142498010648\n",
      "MSE : 0.2135946090738038\n"
     ]
    }
   ],
   "source": [
    "#All together \n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5cfb81",
   "metadata": {},
   "source": [
    "## ANN Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1543e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 1s 2ms/step\n",
      "Adjusted R2 : 0.7861675787959927\n",
      "MSE : 0.21525221355135135\n"
     ]
    }
   ],
   "source": [
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=8))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "\n",
    "#Compiling ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "\n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "#Fitting ANN\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 15, verbose=0)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad887c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Adjusted R2 : 0.6853160650300016\n",
      "MSE : 0.32020098633902827+- 13.642704474237583%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    #Initialising ANN\n",
    "    ann = tf.keras.models.Sequential()\n",
    "\n",
    "    #Adding First Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=8))\n",
    "\n",
    "    #Adding Second Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "    #Adding Output Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    #Compiling ANN\n",
    "    ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "    \n",
    "    #Fitting ANN\n",
    "    ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose=0)\n",
    "\n",
    "    y_pred = ann.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0048afb",
   "metadata": {},
   "source": [
    "## ANN Non-Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df334a8b",
   "metadata": {},
   "source": [
    "Tanh activation on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acae527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 1s 2ms/step\n",
      "Adjusted R2 : 0.7395269761195613\n",
      "MSE : 0.2622024978484775\n"
     ]
    }
   ],
   "source": [
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=8))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation = \"tanh\"))\n",
    "\n",
    "\n",
    "#Compiling ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "\n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "#Fitting ANN\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 15, verbose=0)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86634c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 273us/step\n",
      "10/10 [==============================] - 0s 166us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 682us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "Adjusted R2 : 0.623653216446037\n",
      "MSE : 0.38481438071174645+- 7.983286355297987%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    #Initialising ANN\n",
    "    ann = tf.keras.models.Sequential()\n",
    "\n",
    "    #Adding First Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=8))\n",
    "\n",
    "    #Adding Second Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "    #Adding Output Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=1, activation = \"tanh\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #Compiling ANN\n",
    "    ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "    \n",
    "    #Fitting ANN\n",
    "    ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose=0)\n",
    "\n",
    "    y_pred = ann.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb614",
   "metadata": {},
   "source": [
    "relu activation on the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1ddb7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 1s 3ms/step\n",
      "Adjusted R2 : 0.8513771713056267\n",
      "MSE : 0.14960964609854793\n"
     ]
    }
   ],
   "source": [
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=8, activation = \"relu\"))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "\n",
    "#Compiling ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "\n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "#Fitting ANN\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose=0)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2250934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Adjusted R2 : 0.6728010548014263\n",
      "MSE : 0.33334842761270117+- 11.401132157310542%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    #Initialising ANN\n",
    "    ann = tf.keras.models.Sequential()\n",
    "\n",
    "    #Adding First Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=8, activation = \"relu\"))\n",
    "\n",
    "    #Adding Second Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "    #Adding Output Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    \n",
    "    \n",
    "    #Compiling ANN\n",
    "    ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "    \n",
    "    #Fitting ANN\n",
    "    ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose=0)\n",
    "\n",
    "    y_pred = ann.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9b42e",
   "metadata": {},
   "source": [
    "## Non linear ANN with dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c979891d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "239/239 [==============================] - 2s 3ms/step - loss: 0.8555\n",
      "Epoch 2/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.4274\n",
      "Epoch 3/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.3556\n",
      "Epoch 4/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.3243\n",
      "Epoch 5/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2954\n",
      "Epoch 6/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2913\n",
      "Epoch 7/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2815\n",
      "Epoch 8/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2710\n",
      "Epoch 9/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2625\n",
      "Epoch 10/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2569\n",
      "Epoch 11/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2568\n",
      "Epoch 12/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2555\n",
      "Epoch 13/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2507\n",
      "Epoch 14/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2469\n",
      "Epoch 15/100\n",
      "239/239 [==============================] - 1s 2ms/step - loss: 0.2492\n",
      "Epoch 16/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2454\n",
      "Epoch 17/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2408\n",
      "Epoch 18/100\n",
      "239/239 [==============================] - 1s 2ms/step - loss: 0.2438\n",
      "Epoch 19/100\n",
      "239/239 [==============================] - 1s 2ms/step - loss: 0.2459\n",
      "Epoch 20/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2419\n",
      "Epoch 21/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2388\n",
      "Epoch 22/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2452\n",
      "Epoch 23/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2374\n",
      "Epoch 24/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2433\n",
      "Epoch 25/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2400\n",
      "Epoch 26/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2372\n",
      "Epoch 27/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2409\n",
      "Epoch 28/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2373\n",
      "Epoch 29/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2312\n",
      "Epoch 30/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2341\n",
      "Epoch 31/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2412\n",
      "Epoch 32/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2398\n",
      "Epoch 33/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2410\n",
      "Epoch 34/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2366\n",
      "Epoch 35/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2362\n",
      "Epoch 36/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2345\n",
      "Epoch 37/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2322\n",
      "Epoch 38/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2322\n",
      "Epoch 39/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2349\n",
      "Epoch 40/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2297\n",
      "Epoch 41/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2384\n",
      "Epoch 42/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2321\n",
      "Epoch 43/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2315\n",
      "Epoch 44/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2357\n",
      "Epoch 45/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2331\n",
      "Epoch 46/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2343\n",
      "Epoch 47/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2361\n",
      "Epoch 48/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2310\n",
      "Epoch 49/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2336\n",
      "Epoch 50/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2324\n",
      "Epoch 51/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2304\n",
      "Epoch 52/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2282\n",
      "Epoch 53/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2294\n",
      "Epoch 54/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2345\n",
      "Epoch 55/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2319\n",
      "Epoch 56/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2299\n",
      "Epoch 57/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2285\n",
      "Epoch 58/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2272\n",
      "Epoch 59/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2320\n",
      "Epoch 60/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2285\n",
      "Epoch 61/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2297\n",
      "Epoch 62/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2346\n",
      "Epoch 63/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2267\n",
      "Epoch 64/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2208\n",
      "Epoch 65/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2309\n",
      "Epoch 66/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2250\n",
      "Epoch 67/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2263\n",
      "Epoch 68/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2274\n",
      "Epoch 69/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2299\n",
      "Epoch 70/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2313\n",
      "Epoch 71/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2301\n",
      "Epoch 72/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2310\n",
      "Epoch 73/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2289\n",
      "Epoch 74/100\n",
      "239/239 [==============================] - 1s 2ms/step - loss: 0.2294\n",
      "Epoch 75/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2275\n",
      "Epoch 76/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2334\n",
      "Epoch 77/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2326\n",
      "Epoch 78/100\n",
      "239/239 [==============================] - 1s 4ms/step - loss: 0.2337\n",
      "Epoch 79/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2333\n",
      "Epoch 80/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2283\n",
      "Epoch 81/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2291\n",
      "Epoch 82/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2234\n",
      "Epoch 83/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2291\n",
      "Epoch 84/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2250\n",
      "Epoch 85/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2267\n",
      "Epoch 86/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2266\n",
      "Epoch 87/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2296\n",
      "Epoch 88/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2284\n",
      "Epoch 89/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2309\n",
      "Epoch 90/100\n",
      "239/239 [==============================] - 1s 3ms/step - loss: 0.2277\n",
      "Epoch 91/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2262\n",
      "Epoch 92/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2299\n",
      "Epoch 93/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2263\n",
      "Epoch 94/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2302\n",
      "Epoch 95/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2253\n",
      "Epoch 96/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2274\n",
      "Epoch 97/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2240\n",
      "Epoch 98/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2261\n",
      "Epoch 99/100\n",
      "239/239 [==============================] - 1s 6ms/step - loss: 0.2251\n",
      "Epoch 100/100\n",
      "239/239 [==============================] - 1s 5ms/step - loss: 0.2290\n",
      "239/239 [==============================] - 1s 4ms/step\n",
      "Adjusted R2 : 0.825470171587984\n",
      "MSE : 0.17568866163930402\n"
     ]
    }
   ],
   "source": [
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=8, activation = \"relu\"))\n",
    "\n",
    "ann.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "\n",
    "#Compiling ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "\n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "#Fitting ANN\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 100)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1773d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    #Initialising ANN\n",
    "    ann = tf.keras.models.Sequential()\n",
    "\n",
    "    #Adding First Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=8))\n",
    "\n",
    "    ann.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Adding Second Hidden Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "    #Adding Output Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=1, activation = \"tanh\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #Compiling ANN\n",
    "    ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "    \n",
    "    #Fitting ANN\n",
    "    ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose=0)\n",
    "\n",
    "    y_pred = ann.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1eebc",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3269fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2 : 0.9605589685626578\n",
      "MSE : 0.039702909754441205\n"
     ]
    }
   ],
   "source": [
    "#One model\n",
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 4, alpha = 4, n_estimators = 300)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c86a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2 : 0.706939583177193\n",
      "MSE : 0.29849921251596695+- 9.722567620012493%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                    max_depth = 4, alpha = 4, n_estimators = 300)\n",
    "\n",
    "    xg_reg.fit(X_train, y_train)\n",
    "    y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "\n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd87a6",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e69acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "239/239 [==============================] - 5s 11ms/step - loss: 0.6268\n",
      "Epoch 2/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4968\n",
      "Epoch 3/100\n",
      "239/239 [==============================] - 3s 10ms/step - loss: 0.4577\n",
      "Epoch 4/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4279\n",
      "Epoch 5/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4122\n",
      "Epoch 6/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4092\n",
      "Epoch 7/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4173\n",
      "Epoch 8/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4230\n",
      "Epoch 9/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4156\n",
      "Epoch 10/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4093\n",
      "Epoch 11/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4149\n",
      "Epoch 12/100\n",
      "239/239 [==============================] - 3s 10ms/step - loss: 0.4186\n",
      "Epoch 13/100\n",
      "239/239 [==============================] - 3s 10ms/step - loss: 0.4008\n",
      "Epoch 14/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3992\n",
      "Epoch 15/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3988\n",
      "Epoch 16/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4020\n",
      "Epoch 17/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3921\n",
      "Epoch 18/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3940\n",
      "Epoch 19/100\n",
      "239/239 [==============================] - 3s 10ms/step - loss: 0.3772\n",
      "Epoch 20/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3848\n",
      "Epoch 21/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3808\n",
      "Epoch 22/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3796\n",
      "Epoch 23/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3834\n",
      "Epoch 24/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3783\n",
      "Epoch 25/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3719\n",
      "Epoch 26/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3786\n",
      "Epoch 27/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3844\n",
      "Epoch 28/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3714\n",
      "Epoch 29/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3689\n",
      "Epoch 30/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3690\n",
      "Epoch 31/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3699\n",
      "Epoch 32/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3630\n",
      "Epoch 33/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3676\n",
      "Epoch 34/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3699\n",
      "Epoch 35/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3616\n",
      "Epoch 36/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3539\n",
      "Epoch 37/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3584\n",
      "Epoch 38/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3663\n",
      "Epoch 39/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3580\n",
      "Epoch 40/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3568\n",
      "Epoch 41/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3549\n",
      "Epoch 42/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3480\n",
      "Epoch 43/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3421\n",
      "Epoch 44/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3552\n",
      "Epoch 45/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3606\n",
      "Epoch 46/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3552\n",
      "Epoch 47/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3478\n",
      "Epoch 48/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3466\n",
      "Epoch 49/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3493\n",
      "Epoch 50/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3633\n",
      "Epoch 51/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3514\n",
      "Epoch 52/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3492\n",
      "Epoch 53/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3425\n",
      "Epoch 54/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3461\n",
      "Epoch 55/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3456\n",
      "Epoch 56/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3391\n",
      "Epoch 57/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3331\n",
      "Epoch 58/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3425\n",
      "Epoch 59/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3323\n",
      "Epoch 60/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3415\n",
      "Epoch 61/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3281\n",
      "Epoch 62/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3271\n",
      "Epoch 63/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3333\n",
      "Epoch 64/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3393\n",
      "Epoch 65/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3400\n",
      "Epoch 66/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3311\n",
      "Epoch 67/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3236\n",
      "Epoch 68/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3308\n",
      "Epoch 69/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3285\n",
      "Epoch 70/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3217\n",
      "Epoch 71/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3259\n",
      "Epoch 72/100\n",
      "239/239 [==============================] - 3s 14ms/step - loss: 0.3253\n",
      "Epoch 73/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3165\n",
      "Epoch 74/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3327\n",
      "Epoch 75/100\n",
      "239/239 [==============================] - 3s 13ms/step - loss: 0.3226\n",
      "Epoch 76/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3120\n",
      "Epoch 77/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3094\n",
      "Epoch 78/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3124\n",
      "Epoch 79/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3269\n",
      "Epoch 80/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3080\n",
      "Epoch 81/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3103\n",
      "Epoch 82/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3229\n",
      "Epoch 83/100\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.3196\n",
      "Epoch 84/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3060\n",
      "Epoch 85/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3172\n",
      "Epoch 86/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3090\n",
      "Epoch 87/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3101\n",
      "Epoch 88/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3028\n",
      "Epoch 89/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3083\n",
      "Epoch 90/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3026\n",
      "Epoch 91/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3141\n",
      "Epoch 92/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.2981\n",
      "Epoch 93/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.2972\n",
      "Epoch 94/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3026\n",
      "Epoch 95/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3067\n",
      "Epoch 96/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3130\n",
      "Epoch 97/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3064\n",
      "Epoch 98/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.3041\n",
      "Epoch 99/100\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.2964\n",
      "Epoch 100/100\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.3113\n",
      "239/239 [==============================] - 2s 3ms/step\n",
      "Adjusted R2 : 0.8282501456183259\n",
      "MSE : 0.1728902292955227\n"
     ]
    }
   ],
   "source": [
    "X_train = tot_train.loc[:, tot_train.columns != 'pred']\n",
    "X_test = tot_test.loc[:, tot_test.columns != 'pred']\n",
    "\n",
    "\n",
    "y_train = tot_train.loc[:, tot_train.columns == 'pred']\n",
    "y_test = tot_test.loc[:, tot_test.columns == 'pred']\n",
    "\n",
    "\n",
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#Adding First Hidden Layer\n",
    "ann.add(LSTM(units=8, input_shape = (X_train.shape[1],1), return_sequences=True,activation=\"tanh\",recurrent_activation=\"sigmoid\"))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "ann.add(LSTM(units=4, activation=\"tanh\",recurrent_activation=\"sigmoid\"))\n",
    "\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "#Compiling ANN\n",
    "ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "\n",
    "#Fitting ANN\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 100)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "r2_adj = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78117819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 4ms/step\n",
      "0\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "1\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "3\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "4\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "5\n",
      "10/10 [==============================] - 1s 5ms/step\n",
      "6\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "7\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "8\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "9\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "10\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "11\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "12\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "13\n",
      "10/10 [==============================] - 1s 2ms/step\n",
      "14\n",
      "10/10 [==============================] - 1s 5ms/step\n",
      "15\n",
      "10/10 [==============================] - 1s 5ms/step\n",
      "16\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "17\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "18\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "19\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "20\n",
      "10/10 [==============================] - 2s 5ms/step\n",
      "21\n",
      "10/10 [==============================] - 2s 5ms/step\n",
      "22\n",
      "10/10 [==============================] - 2s 5ms/step\n",
      "23\n",
      "10/10 [==============================] - 2s 5ms/step\n",
      "24\n",
      "Adjusted R2 : 0.6175541631323752\n",
      "MSE : 0.3902705381024586+- 12.209203515321617%\n"
     ]
    }
   ],
   "source": [
    "#One model per household\n",
    "r_2 = [None]*len(dfs_train)\n",
    "mserror = [None]*len(dfs_train)\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    X_train = dfs_train[i].loc[:, dfs_train[i].columns != 'pred']\n",
    "    X_test = dfs_test[i].loc[:, dfs_test[i].columns != 'pred']\n",
    "\n",
    "\n",
    "    y_train = dfs_train[i].loc[:, dfs_train[i].columns == 'pred']\n",
    "    y_test = dfs_test[i].loc[:, dfs_test[i].columns == 'pred']\n",
    "\n",
    "    #Initialising ANN\n",
    "    ann = tf.keras.models.Sequential()\n",
    "\n",
    "    #Adding First Hidden Layer\n",
    "    ann.add(LSTM(units=8, input_shape = (X_train.shape[1],1), return_sequences=True,activation=\"tanh\",recurrent_activation=\"sigmoid\"))\n",
    "\n",
    "    #Adding Second Hidden Layer\n",
    "    ann.add(LSTM(units=4, activation=\"tanh\",recurrent_activation=\"sigmoid\"))\n",
    "\n",
    "    #Adding Output Layer\n",
    "    ann.add(tf.keras.layers.Dense(units=1))\n",
    "    \n",
    "    #Compiling ANN\n",
    "    ann.compile(optimizer=\"adam\",loss=\"MeanSquaredError\")\n",
    "    \n",
    "    #Fitting ANN\n",
    "    ann.fit(X_train,y_train,batch_size=32,epochs = 100, verbose = 0)\n",
    "\n",
    "    y_pred = ann.predict(X_test)\n",
    "    \n",
    "    r_2[i] = 1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)/(len(X_test)-len(X_test.columns)-1))\n",
    "    mserror[i] = mean_squared_error(y_test, y_pred)\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "r2_adj = np.mean(r_2)\n",
    "std = (np.std(r_2)/r2_adj)*100\n",
    "mse = np.mean(mserror)\n",
    "\n",
    "print(\"Adjusted R2 : \" + r2_adj.astype(str) + \"\\nMSE : \" + mse.astype(str) + \"+- \" + std.astype(str) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
