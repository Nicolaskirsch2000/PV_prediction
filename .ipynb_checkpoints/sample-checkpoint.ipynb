{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, pickle, os, copy, itertools, sys\n",
    "import torch, scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "from PVDataset import *\n",
    "\n",
    "random_seed = 3\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random_state = np.random.RandomState(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for multiple households\n",
    "* vary the tilt of the installation and the azimuthal orientation\n",
    "* same coordinates and altitude of central Lausanne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographical characteristics of the location\n",
    "latitude=46.520\n",
    "longitude=6.632\n",
    "name='Lausanne'\n",
    "altitude=496\n",
    "timezone='Etc/GMT-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_file = open(\"saved_results/months.pkl\", \"rb\")\n",
    "hours = pickle.load(a_file)\n",
    "hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_file = open(\"saved_results/months.pkl\", \"rb\")\n",
    "months = pickle.load(a_file)\n",
    "type(months[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: generating data for months 3 4 hours 11 12 13 14 15 .\n",
      "using lags : 1 2 8 16 17 18 19 20 21 22 23\n",
      "[INFO] generating data for 25 clients\n",
      "[INFO] saved data for 25 clients\n"
     ]
    }
   ],
   "source": [
    "generate_data = True\n",
    "\n",
    "if generate_data:\n",
    "\n",
    "    a_file = open(\"saved_results/hours.pkl\", \"rb\")\n",
    "    hours = pickle.load(a_file)\n",
    "\n",
    "    a_file = open(\"saved_results/months.pkl\", \"rb\")\n",
    "    months = pickle.load(a_file)\n",
    "    #months = [3,4,5,6,7,8,9]\n",
    "\n",
    "    a_file = open(\"saved_results/Lags_FS.pkl\", \"rb\")\n",
    "    Lags_FS = pickle.load(a_file)\n",
    "\n",
    "    lags=Lags_FS['best_num_adjr2']\n",
    "\n",
    "    print('INFO: generating data for months', *months, 'hours', *hours, '.')\n",
    "    print('using lags :', *lags)\n",
    "\n",
    "\n",
    "    # dataset info: tilt and azimuth distributions\n",
    "    mean_tilt  = latitude\n",
    "    mean_azimuth = 180\n",
    "    sigma_tilt = 15\n",
    "    mean_az  = 180\n",
    "    sigma_az = 45\n",
    "\n",
    "    # task distribution    \n",
    "    mu_t = [mean_tilt, mean_azimuth]\n",
    "    cov_t = np.diag([sigma_tilt**2, sigma_az**2])\n",
    "\n",
    "    # FL info\n",
    "    num_clients = 25\n",
    "\n",
    "    # Configuration w.r.t. data\n",
    "\n",
    "    generate_normalized_data = False\n",
    "\n",
    "    # generate data from each mode\n",
    "    task_environment = PVDataset(mu_t=mu_t, cov_t=cov_t, lags=lags,\n",
    "                                 latitude=latitude, longitude=longitude, altitude=altitude,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "\n",
    "    print('[INFO] generating data for {:2.0f} clients'.format(num_clients))\n",
    "    clients_data, clients_train_ts, clients_valid_ts = task_environment.generate_clients_data(num_clients=num_clients, \n",
    "                                                          months=months, hours=hours)\n",
    "    if 'target' in task_environment.feature_names:\n",
    "        task_environment.feature_names.remove('target')\n",
    "\n",
    "    clients_train_data = [] # training data of all clients\n",
    "    clients_valid_data = [] # validation data of all clients\n",
    "\n",
    "    for n in np.arange(num_clients):\n",
    "        x_obs, y_obs, x_tru, y_tru = clients_data[n]\n",
    "\n",
    "        # normalize\n",
    "        if generate_normalized_data:\n",
    "            # compute mean and std from train\n",
    "            x_mean, y_mean = np.mean(x_obs, axis=0), np.mean(y_obs, axis=0)\n",
    "            x_std, y_std = np.std(x_obs, axis=0) + 1e-8, np.std(y_obs, axis=0) + 1e-8\n",
    "            # normalize\n",
    "            clients_train_data.append(((x_obs-x_mean)/x_std, (y_obs-y_mean)/y_std))\n",
    "            clients_valid_data.append(((x_tru-x_mean)/x_std, (y_tru-y_mean)/y_std))\n",
    "            clients_train_ts[n]['p_mp'] = (list(clients_train_ts[n]['p_mp'])-y_mean)/y_std\n",
    "            clients_valid_ts[n]['p_mp'] = (list(clients_valid_ts[n]['p_mp'])-y_mean)/y_std\n",
    "            clients_data[n] = (clients_train_data[n][0], clients_train_data[n][1],\n",
    "                               clients_valid_data[n][0],  clients_valid_data[n][1])\n",
    "        else:\n",
    "            clients_train_data.append((x_obs, y_obs))\n",
    "            clients_valid_data.append((x_tru, y_tru))\n",
    "\n",
    "    # Save\n",
    "    env_dict = {'hours':hours, 'months':months, 'mu_t':mu_t,\n",
    "                'cov_t':cov_t, 'num_clients':num_clients, \n",
    "                'clients_train_data':clients_train_data, \n",
    "                'clients_valid_data':clients_valid_data,\n",
    "                'clients_train_ts':clients_train_ts, \n",
    "                'clients_valid_ts':clients_valid_ts,\n",
    "                'generate_normalized_data': generate_normalized_data}\n",
    "    # dump env dict\n",
    "    file1 = open('saved_results/env_dict', 'wb')\n",
    "    pickle.dump(env_dict, file1)\n",
    "    # dump clients data\n",
    "    file2 = open('saved_results/clients_data_full', 'wb')\n",
    "    pickle.dump(clients_data, file2)\n",
    "    # save as .mat\n",
    "    scipy.io.savemat('saved_results/clients_data.mat', {'clients_data': clients_data})\n",
    "    print('[INFO] saved data for {:2.0f} clients'.format(num_clients))\n",
    "else:\n",
    "    file1 = open('saved_results/env_dict', 'rb')\n",
    "    env_dict = pickle.load(file1)\n",
    "    for key, value in env_dict.items(): #TODO: not sure if works for lists\n",
    "        locals()[key]=value\n",
    "    file2 = open('saved_results/clients_data', 'rb')\n",
    "    clients_data = pickle.load(file2)\n",
    "    num_clients = len(clients_data)\n",
    "    print('[INFO] loaded data for {:2.0f} clients'.format(num_clients))\n",
    "# close the file\n",
    "file1.close()\n",
    "file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number of modes =  2, number of clients = 25\n",
      "[INFO] clients training samples =  305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305 305\n",
      "[INFO] clients valid samples =  303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303 303\n",
      "[INFO] number of features = 14, features: H_sun T2m WS10m dayofy_y lag 1 lag 2 lag 8 lag 17 lag 18 lag 19 lag 20 lag 21 lag 22 lag 23\n"
     ]
    }
   ],
   "source": [
    "num_features = len(task_environment.feature_names)\n",
    "num_train_samples = [None]*num_clients\n",
    "num_valid_samples  = [None]*num_clients\n",
    "\n",
    "for client_num in np.arange(num_clients):\n",
    "    # train\n",
    "    (x, y) = clients_train_data[client_num]\n",
    "    assert x.shape[1] == len(task_environment.feature_names)\n",
    "    assert x.shape[0] == len(y)\n",
    "    num_train_samples[client_num] = x.shape[0]\n",
    "    # valid\n",
    "    (x, y) = clients_valid_data[client_num]\n",
    "    assert x.shape[1] == len(task_environment.feature_names)\n",
    "    assert x.shape[0] == len(y)\n",
    "    num_valid_samples[client_num] = x.shape[0]\n",
    "    \n",
    "print('[INFO] number of modes = {:2.0f}, number of clients = {:2.0f}'.format(len(mu_t), num_clients))\n",
    "print('[INFO] clients training samples = ', *num_train_samples)\n",
    "print('[INFO] clients valid samples = ', *num_valid_samples)\n",
    "print('[INFO] number of features = {:2.0f}, features:'.format(len(task_environment.feature_names)), *task_environment.feature_names)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ts(client_ts, pred_mean=None, pred_std=None, title=None,\n",
    "                 selected_months=None, hours = None, figsize=(16, 6)):\n",
    "\n",
    "    '''\n",
    "    visulaize predictions for one client\n",
    "    '''\n",
    "    # check predictions provided for all clients\n",
    "    #if (pred_mean is not None) or (pred_std is not None):\n",
    "    #    assert (pred_mean is not None) and (pred_std is not None)\n",
    "    #    assert len(pred_mean) == len(pred_std)\n",
    "   \n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "\n",
    "    if selected_months is not None:\n",
    "        client_ts = client_ts.loc[client_ts['month'].isin(selected_months), :]\n",
    "    client_ts = client_ts.reset_index()\n",
    "    ax.plot(list(client_ts.p_mp), c='b', lw=1, label='true')\n",
    "    # TODO label with hours\n",
    "    ax.set_xlabel('time (h)')\n",
    "    ax.set_xlabel('generated power (kWh/h)')\n",
    "    if not title is None:\n",
    "        ax.set_title(title) \n",
    "    # plot predictions\n",
    "    if pred_mean is not None and hours is not None:\n",
    "        # get indexes of the ts that correspond to predicted steps\n",
    "        ind = client_ts[client_ts['hour_day'].isin(hours)].index.tolist()\n",
    "        ind = ind[0:pred_mean.shape[0]] # TODO: this is a bug in test_ts. ind and pred means must be the same size\n",
    "        #ind = [i-1 for i in ind] \n",
    "        ax.scatter(ind, pred_mean, color='r', s=5, label='prediction')\n",
    "        if pred_std is not None:\n",
    "            lb = list(client_ts.p_mp)\n",
    "            ub = list(client_ts.p_mp)\n",
    "            for i in np.arange(len(ind)): \n",
    "                lb[ind[i]] = pred_mean[i] - 1.645 * pred_std[i]\n",
    "                ub[ind[i]] = pred_mean[i] + 1.645 * pred_std[i]\n",
    "            ax.fill_between(np.arange(len(lb)), lb, ub, label = 'confidence', color='red',\n",
    "             alpha=0.5)\n",
    "    \n",
    "\n",
    "for client_num in np.arange(num_clients):\n",
    "    title = 'Predictions on training data for client {:2.0f}'.format(client_num)\n",
    "    _, predictions = clients_train_data[client_num] # assume an ideal model with zero error, in practice, this must be the output of your model on clients_train_data[client_num][0] \n",
    "    visualize_ts(client_ts=clients_train_ts[client_num], pred_mean=predictions, selected_months=months, hours=hours, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_red = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if generate_data_red:\n",
    "\n",
    "# reduced size data\n",
    "\n",
    "m_train = 50\n",
    "\n",
    "m_valid = 50\n",
    "\n",
    "use_2y = False # selecr train and validation from both years combined\n",
    "\n",
    "# if False, picks train from 2018 and validation from 1029\n",
    "\n",
    "#c_data = clients_data_2y if use_2y else clients_data_full\n",
    "\n",
    "c_data = clients_data_full\n",
    "\n",
    "clients_data_red = [None] * num_clients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for client_num in np.arange(num_clients):\n",
    "\n",
    "data = c_data[client_num]\n",
    "\n",
    "# randomly select train_inds and valid_inds\n",
    "\n",
    "train_inds = random_state.choice(np.arange(data[0].shape[0]),\n",
    "\n",
    "size=m_train, replace=False)\n",
    "\n",
    "train_inds.sort()\n",
    "\n",
    "valid_inds = random_state.choice(np.arange(data[3].shape[0]),\n",
    "\n",
    "size=m_valid, replace=False)\n",
    "\n",
    "valid_inds.sort()\n",
    "\n",
    "# new train and validation data\n",
    "\n",
    "clients_data_red[client_num] = (data[0][train_inds, :], data[1][train_inds],\n",
    "\n",
    "data[2][valid_inds, :], data[3][valid_inds])\n",
    "\n",
    "# mark samples used for train or validation\n",
    "\n",
    "is_test_2018 = [True]*len(clients_ts_2018[client_num].index)\n",
    "# mark all as test\n",
    "\n",
    "for i in train_inds:\n",
    "\n",
    "is_test_2018[i] = False # train points are not test\n",
    "\n",
    "is_test_2019 = [True]*len(clients_ts_2019[client_num].index)\n",
    "# mark all as test\n",
    "\n",
    "for i in valid_inds:\n",
    "\n",
    "is_test_2019[i] = False # valid points are not test\n",
    "\n",
    "clients_ts_2018[client_num]['is_test'] = is_test_2018 \n",
    "\n",
    "clients_ts_2019[client_num]['is_test'] = is_test_2019 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dump clients data\n",
    "\n",
    "file = open(filename_data+'_red',\n",
    "'wb')\n",
    "\n",
    "pickle.dump(clients_data_red, file)\n",
    "\n",
    "#scipy.io.savemat(filename_data+'_red.mat', {'clients_data_red': clients_data_red}) #TODO\n",
    "\n",
    "else:\n",
    "\n",
    "file = open(filename_data+'_red',\n",
    "'rb')\n",
    "\n",
    "clients_data_red = pickle.load(file)\n",
    "\n",
    "m_train = clients_data_red[0][0].shape[0]\n",
    "\n",
    "m_valid = clients_data_red[0][2].shape[0]\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
